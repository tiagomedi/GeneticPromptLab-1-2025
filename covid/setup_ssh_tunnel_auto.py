#!/usr/bin/env python3
"""
Script para ejecutar Llama3 directamente en el servidor remoto v√≠a SSH
"""

import json
import os
import sys
import time
import paramiko
from typing import Dict, Any, Tuple, Optional

class RemoteSSHExecutor:
    """Clase para ejecutar comandos remotamente v√≠a SSH"""
    
    def __init__(self, credentials_file: str = "ssh_credentials.json"):
        self.credentials_file = credentials_file
        self.credentials = None
        self.jump_client = None
        self.target_client = None
        self.channel = None
        
        self._load_credentials()
        
    def _load_credentials(self) -> bool:
        """Cargar credenciales desde archivo"""
        try:
            with open(self.credentials_file, 'r') as f:
                self.credentials = json.load(f)
            print("‚úÖ Credenciales cargadas desde ssh_credentials.json")
            return True
        except Exception as e:
            print(f"‚ùå Error cargando credenciales: {e}")
            return False
    
    def connect(self) -> bool:
        """Establecer conexi√≥n SSH"""
        if not self.credentials:
            return False
            
        print("üîó Conectando al servidor remoto...")
        
        try:
            ssh_config = self.credentials['ssh_config']
            jump_host = ssh_config['jump_host']
            target_host = ssh_config['target_host']
            
            print("‚úÖ Credenciales verificadas correctamente")
            print(f"   Jump host: {jump_host['username']}@{jump_host['host']}:{jump_host['port']}")
            print(f"   Target host: {target_host['username']}@{target_host['host']}")
            print(f"   Modelo: {self.credentials['model_config']['model']}")
            
            # Conectar al jump host
            self.jump_client = paramiko.SSHClient()
            self.jump_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            self.jump_client.connect(
                hostname=jump_host['host'],
                port=jump_host['port'],
                username=jump_host['username'],
                password=jump_host['password']
            )
            
            # Configurar el t√∫nel
            jump_transport = self.jump_client.get_transport()
            dest_addr = (target_host['host'], target_host['port'])
            local_addr = ('', 0)  # bind to any local port
            self.channel = jump_transport.open_channel(
                "direct-tcpip", dest_addr, local_addr
            )
            
            # Conectar al target host
            self.target_client = paramiko.SSHClient()
            self.target_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            self.target_client.connect(
                hostname=target_host['host'],
                port=target_host['port'],
                username=target_host['username'],
                password=target_host['password'],
                sock=self.channel
            )
            
            print("‚úÖ Conexi√≥n SSH establecida")
            return True
            
        except Exception as e:
            print(f"‚ùå Error de conexi√≥n: {e}")
            self.close()
            return False
    
    def run_command(self, command: str, timeout: int = 30) -> Tuple[bool, str]:
        """Ejecutar comando en el servidor remoto"""
        if not self.target_client:
            return False, "No hay conexi√≥n SSH"
            
        try:
            stdin, stdout, stderr = self.target_client.exec_command(command, timeout=timeout)
            
            # Para comandos de ollama, necesitamos manejar la salida de manera diferente
            if "ollama run" in command:
                # Dar tiempo para que ollama procese
                time.sleep(2)
                
            output = stdout.read().decode()
            error = stderr.read().decode()
            
            if error and "warning" not in error.lower():
                print(f"‚ö†Ô∏è Error: {error}")
                return False, error
                
            return True, output
            
        except Exception as e:
            print(f"‚ùå Error ejecutando comando: {e}")
            return False, str(e)
    
    def test_llama_interactive(self) -> bool:
        """Probar Llama3 usando un enfoque interactivo"""
        print("   üß™ Probando ejecuci√≥n interactiva...")
        
        try:
            # Crear una sesi√≥n interactiva
            channel = self.target_client.invoke_shell()
            
            # Enviar comando
            channel.send("ollama run llama3.1\n")
            time.sleep(3)  # Esperar a que se inicie
            
            # Enviar prompt simple
            channel.send("Hi\n")
            time.sleep(5)  # Esperar respuesta
            
            # Leer respuesta
            output = ""
            while channel.recv_ready():
                output += channel.recv(1024).decode()
            
            # Salir
            channel.send("exit\n")
            channel.close()
            
            if output and len(output) > 10:
                print("‚úÖ Llama3 est√° funcionando correctamente")
                print(f"   Respuesta de prueba:\n{output[-200:]}")
                return True
            else:
                print("‚ùå No se recibi√≥ respuesta v√°lida")
                return False
                
        except Exception as e:
            print(f"‚ùå Error en prueba interactiva: {e}")
            return False
    
    def test_llama(self) -> bool:
        """Probar que Llama3 est√° funcionando"""
        print("\nüß™ Probando Llama3...")
        
        # Verificar que Ollama est√° instalado
        success, output = self.run_command("which ollama")
        if not success or "ollama" not in output:
            print("‚ùå Ollama no est√° instalado en el servidor")
            return False
            
        ollama_path = output.strip()
        print(f"‚úÖ Ollama encontrado en: {ollama_path}")
        
        # Verificar estado del servicio
        print("   üîç Verificando servicio Ollama...")
        success, output = self.run_command("ps aux | grep ollama | grep -v grep")
        
        if not success or not output:
            print("‚ö†Ô∏è Servicio Ollama no est√° activo")
            print("   üîÑ Iniciando servicio...")
            success, _ = self.run_command(f"{ollama_path} serve &")
            if not success:
                print("‚ùå Error iniciando servicio Ollama")
                return False
                
            # Esperar a que el servicio est√© listo
            print("   ‚è≥ Esperando a que el servicio est√© listo...")
            time.sleep(30)  # Esperar 30 segundos
        else:
            print("‚úÖ Servicio Ollama est√° activo")
            
        # Verificar que Llama3 est√° disponible
        success, output = self.run_command(f"{ollama_path} list")
        if not success:
            print("‚ùå Error verificando modelos disponibles")
            return False
            
        if "llama3.1" not in output.lower():
            print("‚ùå Modelo llama3.1 no est√° disponible")
            print("   üîÑ Instalando llama3.1 (esto puede tomar varios minutos)...")
            success, _ = self.run_command(f"{ollama_path} pull llama3.1", timeout=600)
            if not success:
                print("‚ùå Error instalando llama3.1")
                return False
            
            # Verificar instalaci√≥n
            success, output = self.run_command(f"{ollama_path} list")
            if not success or "llama3.1" not in output.lower():
                print("‚ùå Error verificando instalaci√≥n de llama3.1")
                return False
        
        # Probar Llama3 con m√©todo interactivo
        return self.test_llama_interactive()
    
    def run_ollama_structured_command(self, payload: Dict[str, Any], timeout: int = 30) -> Tuple[bool, str]:
        """Ejecutar un comando estructurado con ollama usando el formato JSON especificado"""
        try:
            # Extraer informaci√≥n del payload
            modelo = payload.get("model", "llama3.1")
            messages = payload.get("messages", [])
            temperatura = payload.get("temperature", 0.7)
            
            # Construir el prompt completo a partir de los mensajes
            prompt_parts = []
            for message in messages:
                role = message.get("role", "")
                content = message.get("content", "")
                
                if role == "system":
                    prompt_parts.append(f"System: {content}")
                elif role == "user":
                    prompt_parts.append(f"User: {content}")
                elif role == "assistant":
                    prompt_parts.append(f"Assistant: {content}")
            
            # Combinar todas las partes del prompt
            full_prompt = "\n\n".join(prompt_parts)
            
            # Crear una sesi√≥n interactiva
            channel = self.target_client.invoke_shell()
            
            # Enviar comando para iniciar ollama con el modelo espec√≠fico
            channel.send(f"ollama run {modelo}\n")
            time.sleep(3)  # Esperar a que se inicie
            
            # Limpiar el buffer
            while channel.recv_ready():
                channel.recv(1024)
            
            # Enviar el prompt estructurado
            channel.send(f"{full_prompt}\n")
            time.sleep(10)  # Esperar m√°s tiempo para la respuesta estructurada
            
            # Leer toda la respuesta
            output = ""
            max_attempts = 15
            attempts = 0
            
            while attempts < max_attempts:
                if channel.recv_ready():
                    data = channel.recv(1024).decode()
                    output += data
                    attempts = 0  # Reset counter if we're still receiving data
                else:
                    time.sleep(0.5)
                    attempts += 1
            
            # Salir de ollama
            channel.send("/bye\n")
            time.sleep(1)
            channel.close()
            
            # Extraer solo la respuesta generada
            if output:
                # Buscar la respuesta despu√©s del prompt
                lines = output.split('\n')
                response_started = False
                response_lines = []
                
                for line in lines:
                    # Si encontramos partes del prompt, comenzamos a buscar la respuesta
                    if not response_started and ("User:" in line or "Assistant:" in line):
                        response_started = True
                        continue
                    
                    # Si ya comenzamos a recolectar y encontramos el prompt de ollama, paramos
                    if response_started and (">>>" in line or "Send a message" in line):
                        break
                    
                    # Recolectar l√≠neas de respuesta
                    if response_started and line.strip():
                        # Filtrar l√≠neas que no son parte de la respuesta
                        if not any(marker in line for marker in [">>>", "Send a message", "Use /bye"]):
                            response_lines.append(line.strip())
                
                response = '\n'.join(response_lines).strip()
                
                # Limpiar respuesta de artefactos comunes
                response = response.replace(">>> ", "").replace("Send a message (/? for help)", "")
                
                if response and len(response) > 10:  # Asegurar que hay contenido real
                    return True, response
                else:
                    return False, "Respuesta vac√≠a o muy corta"
            else:
                return False, "No se recibi√≥ salida"
                
        except Exception as e:
            print(f"‚ùå Error ejecutando comando ollama estructurado: {e}")
            return False, str(e)
    
    def run_ollama_command(self, prompt: str, timeout: int = 30) -> Tuple[bool, str]:
        """Ejecutar un comando espec√≠fico con ollama"""
        try:
            # Crear una sesi√≥n interactiva
            channel = self.target_client.invoke_shell()
            
            # Enviar comando para iniciar ollama
            channel.send("ollama run llama3.1\n")
            time.sleep(3)  # Esperar a que se inicie
            
            # Limpiar el buffer
            while channel.recv_ready():
                channel.recv(1024)
            
            # Enviar el prompt
            channel.send(f"{prompt}\n")
            time.sleep(8)  # Esperar m√°s tiempo para la respuesta
            
            # Leer toda la respuesta
            output = ""
            max_attempts = 10
            attempts = 0
            
            while attempts < max_attempts:
                if channel.recv_ready():
                    data = channel.recv(1024).decode()
                    output += data
                    attempts = 0  # Reset counter if we're still receiving data
                else:
                    time.sleep(0.5)
                    attempts += 1
            
            # Salir de ollama
            channel.send("/bye\n")
            time.sleep(1)
            channel.close()
            
            # Extraer solo la respuesta generada (despu√©s del prompt)
            if output:
                # Buscar el patr√≥n del prompt en la salida
                lines = output.split('\n')
                response_started = False
                response_lines = []
                
                for line in lines:
                    # Si encontramos el prompt, comenzamos a recolectar la respuesta
                    if not response_started and prompt.strip() in line:
                        response_started = True
                        continue
                    
                    # Si ya comenzamos a recolectar y encontramos el prompt de ollama, paramos
                    if response_started and (">>>" in line or "Send a message" in line):
                        break
                    
                    # Recolectar l√≠neas de respuesta
                    if response_started and line.strip():
                        response_lines.append(line.strip())
                
                response = '\n'.join(response_lines).strip()
                
                if response and len(response) > 5:  # Asegurar que hay contenido real
                    return True, response
                else:
                    return False, "Respuesta vac√≠a o muy corta"
            else:
                return False, "No se recibi√≥ salida"
                
        except Exception as e:
            print(f"‚ùå Error ejecutando comando ollama: {e}")
            return False, str(e)
    
    def close(self):
        """Cerrar conexiones SSH"""
        print("üßπ Cerrando conexi√≥n SSH...")
        
        if self.target_client:
            self.target_client.close()
            
        if self.channel:
            self.channel.close()
            
        if self.jump_client:
            self.jump_client.close()
            
        print("‚úÖ Conexi√≥n SSH cerrada")

def main():
    """Funci√≥n principal"""
    print("üöÄ Ejecutor Remoto SSH para Llama3")
    print("=" * 50)
    
    executor = RemoteSSHExecutor()
    
    try:
        if executor.connect():
            if executor.test_llama():
                print("\nüéâ ¬°Conexi√≥n y Llama3 funcionando correctamente!")
                print("   Puedes usar esta conexi√≥n para ejecutar comandos remotos.")
                
                # Ejemplo de uso
                print("\nüìù Ejemplo de uso:")
                success, output = executor.run_command('echo "¬øQu√© es COVID-19?" | ollama run llama2')
                if success:
                    print(f"Respuesta:\n{output}")
            else:
                print("\n‚ùå Llama3 no est√° funcionando correctamente")
        else:
            print("\n‚ùå No se pudo establecer la conexi√≥n SSH")
            
    except KeyboardInterrupt:
        print("\nüõë Interrumpido por el usuario")
    finally:
        executor.close()

if __name__ == "__main__":
    main() 